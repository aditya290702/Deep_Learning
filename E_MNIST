import torch
from torch import nn
from torchvision import datasets
from torchvision import transforms
from torch.utils.data import DataLoader
import numpy as np
import torch.optim as optim
from matplotlib import pyplot as plt


Transform = transforms.Compose([transforms.ToTensor()])
Train = datasets.EMNIST(root='./data/EMNIST',
                        download=False,
                        split='byclass',
                        train=True,
                        transform=Transform)

Test = datasets.EMNIST(root='./data/EMNIST',
                        download=False,
                        split='byclass',
                        train = False,
                        transform=Transform)

device = 'cuda' if torch.cuda.is_available() else "cpu"
print()
print(f"{device} is being used")
print("-----------------------------------------------")

batch_size = 16

Train_loader = DataLoader(Train,batch_size,shuffle=True)
Test_loader = DataLoader(Test,batch_size,shuffle=False)

print(f"Length of Train Data : { len(Train_loader)}")
print(f"Length of Train Data : {len(Test_loader)}")
print("-----------------------------------------------")


labels = np.unique(Train.classes)
print(f"The no of Unique classes are : {labels}")
print("-----------------------------------------------")

rows = 10
cols = 6
figure, axes = plt.subplots(rows,cols, figsize= (15,20))

for i in range(1,rows*cols + 1):
    sample_idx = torch.randint(len(Train),size=(1,)).item()
    images , labels = Train[sample_idx]
    figure.add_subplot(rows,cols,i)
    plt.title(labels)
    image_np = images.numpy().squeeze()
    plt.imshow(image_np,cmap="grey")
    plt.axis("off")
plt.tight_layout()
plt.show()

model = nn.Sequential(nn.Flatten(),
                      nn.Linear(28*28,512),
                      nn.ReLU(),

                      nn.Flatten(),
                      nn.Linear(512, 256),
                      nn.ReLU(),

                      nn.Flatten(),
                      nn.Linear(256, 128),
                      nn.ReLU(),

                      nn.Flatten(),
                      nn.Linear(128, 62),
                      nn.ReLU(),
                      ).to(device)

Loss_Criterion = nn.CrossEntropyLoss()
Optim = optim.Adam(model.parameters(),lr=0.001)
print(f"Model Architecture : {model}")
print(f"The {Optim} Optimiser is Being used")
x = torch.randn(1,28,28).to(device)

total_loss = 0

print()
for epochs in range(10):
    for images,labels in Train_loader:
        images,labels = images.to(device),labels.to(device)
        Optim.zero_grad()
        Output = model(images)
        loss = Loss_Criterion(Output,labels)
        loss.backward()
        Optim.step()
    total_loss += loss.item()
    print(f"------------------ Running Epoch : {epochs + 1} --------------------")
    print()
    print(f"Loss : {loss.item()}, Total_loss : {total_loss}")
    print()
